/**
 * Ollama è¿æ¥æµ‹è¯•å·¥å…·
 * 
 * ç”¨äºéªŒè¯ Origin Bridge æ˜¯å¦æ­£å¸¸å·¥ä½œ
 * 
 * ä½¿ç”¨æ–¹æ³•ï¼š
 * 1. åœ¨æµè§ˆå™¨æ§åˆ¶å°ä¸­å¯¼å…¥æ­¤æ¨¡å—
 * 2. è°ƒç”¨ testOllamaConnection()
 */

import { logger } from "@/utils/logger"

const testLogger = logger.withTag("OllamaTest")

interface TestResult {
  success: boolean
  message: string
  details?: any
}

/**
 * æµ‹è¯• Ollama è¿æ¥ï¼ˆæ¨¡å‹åˆ—è¡¨ï¼‰
 */
export async function testOllamaModels(): Promise<TestResult> {
  const endpoints = [
    { name: "OpenAI å…¼å®¹ (/v1/models)", url: "http://localhost:11434/v1/models" },
    { name: "Legacy (/api/tags)", url: "http://localhost:11434/api/tags" }
  ]

  const results: any[] = []

  for (const endpoint of endpoints) {
    testLogger.info(`æµ‹è¯•ç«¯ç‚¹: ${endpoint.name}`)
    testLogger.info(`URL: ${endpoint.url}`)

    try {
      const startTime = Date.now()
      const response = await fetch(endpoint.url, {
        method: "GET",
        headers: {
          "Content-Type": "application/json"
        }
      })

      const latency = Date.now() - startTime

      if (response.ok) {
        const data = await response.json()
        testLogger.info(`âœ… ${endpoint.name} æˆåŠŸ (${latency}ms)`)
        testLogger.info("å“åº”æ•°æ®:", data)
        results.push({
          endpoint: endpoint.name,
          success: true,
          status: response.status,
          latency,
          data
        })
      } else {
        const errorText = await response.text()
        testLogger.error(`âŒ ${endpoint.name} å¤±è´¥ (${response.status})`)
        testLogger.error("é”™è¯¯å“åº”:", errorText)
        results.push({
          endpoint: endpoint.name,
          success: false,
          status: response.status,
          latency,
          error: errorText
        })
      }
    } catch (error) {
      testLogger.error(`âŒ ${endpoint.name} å¼‚å¸¸:`, error)
      results.push({
        endpoint: endpoint.name,
        success: false,
        error: error instanceof Error ? error.message : String(error)
      })
    }
  }

  const allSuccess = results.every(r => r.success)
  return {
    success: allSuccess,
    message: allSuccess ? "æ‰€æœ‰ç«¯ç‚¹æµ‹è¯•é€šè¿‡" : "éƒ¨åˆ†ç«¯ç‚¹æµ‹è¯•å¤±è´¥",
    details: results
  }
}

/**
 * æµ‹è¯• Ollama èŠå¤©æ¥å£
 */
export async function testOllamaChat(): Promise<TestResult> {
  const endpoint = "http://localhost:11434/v1/chat/completions"
  
  testLogger.info("æµ‹è¯•èŠå¤©ç«¯ç‚¹:", endpoint)

  const body = {
    model: "qwen2.5:7b",
    messages: [
      {
        role: "user",
        content: "æµ‹è¯•è¿æ¥ï¼Œè¯·å›å¤ 'OK'"
      }
    ],
    stream: false,
    max_tokens: 10
  }

  testLogger.info("è¯·æ±‚ä½“:", body)

  try {
    const startTime = Date.now()
    const response = await fetch(endpoint, {
      method: "POST",
      headers: {
        "Content-Type": "application/json"
      },
      body: JSON.stringify(body)
    })

    const latency = Date.now() - startTime

    if (response.ok) {
      const data = await response.json()
      testLogger.info(`âœ… èŠå¤©æ¥å£æµ‹è¯•æˆåŠŸ (${latency}ms)`)
      testLogger.info("å“åº”æ•°æ®:", data)
      return {
        success: true,
        message: `æµ‹è¯•æˆåŠŸï¼Œå»¶è¿Ÿ ${latency}ms`,
        details: { latency, data }
      }
    } else {
      const errorText = await response.text()
      testLogger.error(`âŒ èŠå¤©æ¥å£æµ‹è¯•å¤±è´¥ (${response.status})`)
      testLogger.error("é”™è¯¯å“åº”:", errorText)
      return {
        success: false,
        message: `æµ‹è¯•å¤±è´¥: HTTP ${response.status}`,
        details: { status: response.status, error: errorText }
      }
    }
  } catch (error) {
    testLogger.error("âŒ èŠå¤©æ¥å£æµ‹è¯•å¼‚å¸¸:", error)
    return {
      success: false,
      message: "æµ‹è¯•å¼‚å¸¸",
      details: { error: error instanceof Error ? error.message : String(error) }
    }
  }
}

/**
 * å®Œæ•´æµ‹è¯•å¥—ä»¶
 */
export async function testOllamaConnection(): Promise<void> {
  testLogger.info("=".repeat(60))
  testLogger.info("å¼€å§‹ Ollama è¿æ¥æµ‹è¯•")
  testLogger.info("=".repeat(60))

  // æµ‹è¯• 1: æ¨¡å‹åˆ—è¡¨
  testLogger.info("\n[æµ‹è¯• 1] æ¨¡å‹åˆ—è¡¨ç«¯ç‚¹")
  const modelsResult = await testOllamaModels()
  testLogger.info("æµ‹è¯•ç»“æœ:", modelsResult)

  // æµ‹è¯• 2: èŠå¤©æ¥å£
  testLogger.info("\n[æµ‹è¯• 2] èŠå¤©æ¥å£")
  const chatResult = await testOllamaChat()
  testLogger.info("æµ‹è¯•ç»“æœ:", chatResult)

  // æ±‡æ€»
  testLogger.info("\n" + "=".repeat(60))
  testLogger.info("æµ‹è¯•æ±‡æ€»")
  testLogger.info("=".repeat(60))
  testLogger.info("æ¨¡å‹åˆ—è¡¨:", modelsResult.success ? "âœ… é€šè¿‡" : "âŒ å¤±è´¥")
  testLogger.info("èŠå¤©æ¥å£:", chatResult.success ? "âœ… é€šè¿‡" : "âŒ å¤±è´¥")

  if (modelsResult.success && chatResult.success) {
    testLogger.info("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Ollama è¿æ¥æ­£å¸¸")
  } else {
    testLogger.error("\nâš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥:")
    testLogger.error("1. Ollama æœåŠ¡æ˜¯å¦è¿è¡Œ (ollama serve)")
    testLogger.error("2. æ¨¡å‹æ˜¯å¦å·²æ‹‰å– (ollama pull qwen2.5:7b)")
    testLogger.error("3. æ˜¯å¦é…ç½®äº† OLLAMA_ORIGINS ç¯å¢ƒå˜é‡")
    testLogger.error("4. æ‰©å±•æ˜¯å¦å·²é‡æ–°åŠ è½½å¹¶æˆäºˆæƒé™")
  }
}

// åœ¨å¼€å‘æ¨¡å¼ä¸‹è‡ªåŠ¨å¯¼å‡ºåˆ° window
if (process.env.NODE_ENV === 'development' && typeof window !== 'undefined') {
  (window as any).testOllamaConnection = testOllamaConnection;
  (window as any).testOllamaModels = testOllamaModels;
  (window as any).testOllamaChat = testOllamaChat;
  testLogger.info("å·²å°†æµ‹è¯•å‡½æ•°å¯¼å‡ºåˆ° window:")
  testLogger.info("- window.testOllamaConnection()")
  testLogger.info("- window.testOllamaModels()")
  testLogger.info("- window.testOllamaChat()")
}
