/**
 * OpenAI Provider V2 (é‡æ„ç‰ˆ)
 * 
 * ç»§æ‰¿ BaseAIServiceï¼Œåªå®ç° API è°ƒç”¨é€»è¾‘
 * æç¤ºè¯å’Œé€šç”¨é€»è¾‘ç”±åŸºç±»æä¾›
 * 
 * æ”¯æŒçš„æ¨¡å‹ï¼š
 * - gpt-4o-mini: æœ€å¿«æœ€ä¾¿å®œ ($0.150 è¾“å…¥, $0.600 è¾“å‡º / 1M tokens)
 * - gpt-4o: å¼ºå¤§æ¨¡å‹ ($2.50 è¾“å…¥, $10.0 è¾“å‡º / 1M tokens)
 * - o1-mini: æ¨ç†æ¨¡å‹ ($3.0 è¾“å…¥, $12.0 è¾“å‡º / 1M tokens)
 * - o1: é«˜çº§æ¨ç†æ¨¡å‹ ($15.0 è¾“å…¥, $60.0 è¾“å‡º / 1M tokens)
 */

import { BaseAIService } from "../BaseAIService"
import { OpenAICostCalculator, type TokenUsage } from "../CostCalculator"
import type { AIProviderConfig } from "@/types/ai"
import { logger } from "@/utils/logger"

const openaiLogger = logger.withTag("OpenAIProvider")

// ä½¿ç”¨ç»Ÿä¸€çš„æˆæœ¬è®¡ç®—å™¨
const costCalculator = new OpenAICostCalculator()

// æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨ï¼ˆç”¨äºç±»å‹æ£€æŸ¥ï¼‰
const SUPPORTED_MODELS = [
  'gpt-4o', 'gpt-4o-mini',
  'o1', 'o1-mini',
  'gpt-5-nano', 'gpt-5-mini', 'gpt-5', 'o4-mini'
] as const

type OpenAIModel = typeof SUPPORTED_MODELS[number]

type OpenAIResponseFormat =
  | {
      type: "json_object"
    }
  | {
      type: "json_schema"
      json_schema: {
        name: string
        strict?: boolean
        schema: Record<string, unknown>
      }
    }

/**
 * OpenAI API è¯·æ±‚ç±»å‹
 */
interface OpenAIRequest {
  model: string
  messages: Array<{
    role: "user" | "assistant" | "system"
    content: string
  }>
  response_format?: OpenAIResponseFormat
  max_tokens?: number
  temperature?: number
  stream?: boolean
}

/**
 * OpenAI API å“åº”ç±»å‹
 */
interface OpenAIResponse {
  choices: Array<{
    message: {
      role: string
      content: string
    }
    finish_reason: string
  }>
  usage: {
    prompt_tokens: number
    completion_tokens: number
    total_tokens: number
    prompt_tokens_details?: {
      cached_tokens?: number
      audio_tokens?: number
    }
  }
}

export class OpenAIProvider extends BaseAIService {
  readonly name = "OpenAI"
  
  private endpoint = "https://api.openai.com/v1/chat/completions"
  private model: OpenAIModel = "gpt-5-mini"
  private lastUsedModel: OpenAIModel = this.model
  
  // è¿½è¸ªæœ€åä¸€æ¬¡è¯·æ±‚çš„ç¼“å­˜å‘½ä¸­æƒ…å†µï¼ˆç”¨äºç²¾ç¡®è®¡è´¹ï¼‰
  private lastCacheStats: {
    cachedTokens: number
    uncachedTokens: number
  } | null = null
  
  constructor(config: AIProviderConfig) {
    super(config)
    if (config.model && SUPPORTED_MODELS.includes(config.model as OpenAIModel)) {
      this.model = config.model as OpenAIModel
    }
    this.lastUsedModel = this.model
    this.config.model = this.model
    
    if (config.endpoint) {
      this.endpoint = config.endpoint
    }
  }
  
  /**
   * å®ç°ï¼šè°ƒç”¨ OpenAI Chat API
   */
  protected async callChatAPI(
    prompt: string,
    options?: {
      maxTokens?: number
      timeout?: number
      jsonMode?: boolean
      useReasoning?: boolean
      responseFormat?: Record<string, unknown>
      temperature?: number
    }
  ): Promise<{
    content: string
    tokensUsed: {
      input: number
      output: number
    }
    model?: string
  }> {
    const isReasoning = options?.useReasoning ?? false
    const requestModel: OpenAIModel = isReasoning ? "o4-mini" : this.model
    const request: OpenAIRequest = {
      model: requestModel,
      messages: [
        {
          role: "user",
          content: prompt
        }
      ],
      max_tokens: options?.maxTokens || 8000,  // ä½¿ç”¨ 8K ä½œä¸ºé»˜è®¤å€¼ï¼Œé¿å…æˆªæ–­
      temperature: options?.temperature ?? (isReasoning ? undefined : 0.7),
      stream: false
    }
    
    if (options?.responseFormat) {
      request.response_format = options.responseFormat as OpenAIRequest["response_format"]
    } else if (options?.jsonMode) {
      request.response_format = { type: "json_object" }
    }
    
    // æ¨ç†æ¨¡å‹ä¸æ”¯æŒæŸäº›å‚æ•°
    if (isReasoning) {
      delete request.temperature
      delete request.response_format // o1 ç³»åˆ—ä¸æ”¯æŒ JSON Mode
    }
    
    try {
      const response = await fetch(this.endpoint, {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
          "Authorization": `Bearer ${this.config.apiKey}`
        },
        body: JSON.stringify(request),
        signal: options?.timeout 
          ? AbortSignal.timeout(options.timeout)
          : undefined
      })
      
      if (!response.ok) {
        const error = await response.text()
        throw new Error(`OpenAI API error (${response.status}): ${error}`)
      }
      
      const data: OpenAIResponse = await response.json()
      
      if (!data.choices || data.choices.length === 0) {
        throw new Error("OpenAI API returned no choices")
      }
      
      const content = data.choices[0].message.content
      
      // å¯¹äºæ¨ç†æ¨¡å‹ï¼Œå¯èƒ½éœ€è¦ç‰¹æ®Šå¤„ç†
      // o1 æ¨¡å‹ä¼šåœ¨ reasoning å­—æ®µè¿”å›æ€ç»´é“¾ï¼Œä½†æˆ‘ä»¬åªéœ€è¦æœ€ç»ˆç­”æ¡ˆ
      
      this.lastUsedModel = requestModel
      
      // ä¿å­˜ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯ï¼ˆç”¨äºæˆæœ¬è®¡ç®—ï¼‰
      const cachedTokens = data.usage.prompt_tokens_details?.cached_tokens || 0
      this.lastCacheStats = {
        cachedTokens,
        uncachedTokens: data.usage.prompt_tokens - cachedTokens
      }
      
      // æ—¥å¿—è®°å½•ç¼“å­˜å‘½ä¸­æƒ…å†µ
      if (cachedTokens > 0) {
        openaiLogger.debug("ç¼“å­˜ç»Ÿè®¡", {
          cachedTokens,
          uncachedTokens: this.lastCacheStats.uncachedTokens,
          hitRate: (cachedTokens / data.usage.prompt_tokens) * 100
        })
      }
      
      return {
        content,
        tokensUsed: {
          input: data.usage.prompt_tokens,
          output: data.usage.completion_tokens
        },
        model: requestModel
      }
    } catch (error) {
      if (error instanceof Error && error.name === 'AbortError') {
        throw new Error(`OpenAI API timeout (${options?.timeout}ms)`)
      }
      throw error
    }
  }
  
  /**
   * å®ç°ï¼šè·å–è´§å¸ç±»å‹
   */
  protected getCurrency(): 'CNY' | 'USD' | 'FREE' {
    return 'USD'  // OpenAI ä½¿ç”¨ç¾å…ƒ
  }

  /**
   * å®ç°ï¼šè®¡ç®—æˆæœ¬ï¼ˆç¾å…ƒï¼‰
   */
  protected calculateCost(inputTokens: number, outputTokens: number): number {
    const breakdown = this.calculateCostBreakdown(inputTokens, outputTokens)
    return breakdown.input + breakdown.output
  }
  
  /**
   * å®ç°ï¼šè®¡ç®—æˆæœ¬æ˜ç»†ï¼ˆè¾“å…¥å’Œè¾“å‡ºåˆ†å¼€ï¼Œç¾å…ƒï¼‰
   * 
   * ä½¿ç”¨ API è¿”å›çš„çœŸå®ç¼“å­˜å‘½ä¸­æ•°æ®è®¡ç®—æˆæœ¬
   */
  protected calculateCostBreakdown(inputTokens: number, outputTokens: number): { input: number; output: number } {
    // æ„å»º TokenUsage å¯¹è±¡
    const usage: TokenUsage = {
      input: inputTokens,
      output: outputTokens,
      // å¦‚æœæœ‰ç¼“å­˜æ•°æ®ï¼Œä½¿ç”¨çœŸå®çš„ç¼“å­˜å‘½ä¸­æ•°
      cachedInput: this.lastCacheStats?.cachedTokens
    }
    
    // ä½¿ç”¨ç»Ÿä¸€çš„æˆæœ¬è®¡ç®—å™¨
    const result = costCalculator.calculateCost(usage, this.lastUsedModel)
    
    openaiLogger.debug(`ğŸ’° æˆæœ¬è®¡ç®—: ${inputTokens} input (${usage.cachedInput || 0} cached) + ${outputTokens} output = $${result.total.toFixed(6)}`)
    
    return { input: result.input, output: result.output }
  }

  protected getProfileResponseFormat(): Record<string, unknown> | null {
    return {
      type: "json_schema",
      json_schema: {
        name: "user_profile",
        strict: true,
        schema: {
          type: "object",
          additionalProperties: false,
          required: ["interests", "preferences", "avoidTopics"],
          properties: {
            interests: {
              type: "string",
              minLength: 20,
              maxLength: 400
            },
            preferences: {
              type: "array",
              minItems: 3,
              maxItems: 10,
              items: {
                type: "string",
                minLength: 2,
                maxLength: 80
              }
            },
            avoidTopics: {
              type: "array",
              minItems: 0,
              maxItems: 5,
              items: {
                type: "string",
                minLength: 2,
                maxLength: 60
              }
            }
          }
        }
      }
    }
  }
  
  /**
   * æµ‹è¯•è¿æ¥
   */
  async testConnection(): Promise<{
    success: boolean
    message: string
    latency?: number
  }> {
    try {
      const startTime = Date.now()
      
      // ä½¿ç”¨è¶³å¤Ÿå¤§çš„ maxTokens é¿å…è§¦å‘æˆªæ–­è­¦å‘Š
      await this.callChatAPI("æµ‹è¯•è¿æ¥ï¼Œè¯·å›å¤ OK", {
        maxTokens: 200,
        timeout: 10000,
        jsonMode: false
      })
      
      const latency = Date.now() - startTime
      
      return {
        success: true,
        message: `è¿æ¥æˆåŠŸï¼OpenAI API æ­£å¸¸å·¥ä½œ (æ¨¡å‹: ${this.model})`,
        latency
      }
    } catch (error) {
      return {
        success: false,
        message: `è¿æ¥å¤±è´¥: ${error instanceof Error ? error.message : String(error)}`
      }
    }
  }
}
