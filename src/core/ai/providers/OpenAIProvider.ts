/**
 * OpenAI Provider V2 (é‡æ„ç‰ˆ)
 * 
 * ç»§æ‰¿ BaseAIServiceï¼Œåªå®ç° API è°ƒç”¨é€»è¾‘
 * æç¤ºè¯å’Œé€šç”¨é€»è¾‘ç”±åŸºç±»æä¾›
 * 
 * æ”¯æŒçš„æ¨¡å‹ï¼š
 * - gpt-4o-mini: æœ€å¿«æœ€ä¾¿å®œ ($0.150 è¾“å…¥, $0.600 è¾“å‡º / 1M tokens)
 * - gpt-4o: å¼ºå¤§æ¨¡å‹ ($2.50 è¾“å…¥, $10.0 è¾“å‡º / 1M tokens)
 * - o1-mini: æ¨ç†æ¨¡å‹ ($3.0 è¾“å…¥, $12.0 è¾“å‡º / 1M tokens)
 * - o1: é«˜çº§æ¨ç†æ¨¡å‹ ($15.0 è¾“å…¥, $60.0 è¾“å‡º / 1M tokens)
 */

import { BaseAIService } from "../BaseAIService"
import type { AIProviderConfig } from "@/types/ai"
import { logger } from "@/utils/logger"

const openaiLogger = logger.withTag("OpenAIProvider")

/**
 * OpenAI æ¨¡å‹å®šä»·ï¼ˆæ¯ 1M tokensï¼Œç¾å…ƒï¼‰
 * æ•°æ®æ¥æº: https://openai.com/api/pricing/ (2025-11)
 */
const MODEL_PRICING = {
  "gpt-5-nano": {
    input: 0.050,
    inputCached: 0.005,
    output: 0.400
  },
  "gpt-5-mini": {
    input: 0.250,
    inputCached: 0.025,
    output: 2.0
  },
  "gpt-5": {
    input: 1.25,
    inputCached: 0.125,
    output: 10.0
  },
  "o4-mini": {
    input: 4.0,
    inputCached: 1.0,
    output: 16.0
  }
} as const

type OpenAIModel = keyof typeof MODEL_PRICING

type OpenAIResponseFormat =
  | {
      type: "json_object"
    }
  | {
      type: "json_schema"
      json_schema: {
        name: string
        strict?: boolean
        schema: Record<string, unknown>
      }
    }

/**
 * OpenAI API è¯·æ±‚ç±»å‹
 */
interface OpenAIRequest {
  model: string
  messages: Array<{
    role: "user" | "assistant" | "system"
    content: string
  }>
  response_format?: OpenAIResponseFormat
  max_tokens?: number
  temperature?: number
  stream?: boolean
}

/**
 * OpenAI API å“åº”ç±»å‹
 */
interface OpenAIResponse {
  choices: Array<{
    message: {
      role: string
      content: string
    }
    finish_reason: string
  }>
  usage: {
    prompt_tokens: number
    completion_tokens: number
    total_tokens: number
    prompt_tokens_details?: {
      cached_tokens?: number
      audio_tokens?: number
    }
  }
}

export class OpenAIProvider extends BaseAIService {
  readonly name = "OpenAI"
  
  private endpoint = "https://api.openai.com/v1/chat/completions"
  private model: OpenAIModel = "gpt-5-mini"
  private lastUsedModel: OpenAIModel = this.model
  
  // å‡è®¾ç¼“å­˜å‘½ä¸­ç‡ï¼ˆç”¨äºæˆæœ¬ä¼°ç®—ï¼‰
  private readonly CACHE_HIT_RATE = 0.1 // 10%
  
  constructor(config: AIProviderConfig) {
    super(config)
    if (config.model && config.model in MODEL_PRICING) {
      this.model = config.model as OpenAIModel
    }
    this.lastUsedModel = this.model
    this.config.model = this.model
    
    if (config.endpoint) {
      this.endpoint = config.endpoint
    }
  }
  
  /**
   * å®ç°ï¼šè°ƒç”¨ OpenAI Chat API
   */
  protected async callChatAPI(
    prompt: string,
    options?: {
      maxTokens?: number
      timeout?: number
      jsonMode?: boolean
      useReasoning?: boolean
      responseFormat?: Record<string, unknown>
      temperature?: number
    }
  ): Promise<{
    content: string
    tokensUsed: {
      input: number
      output: number
    }
    model?: string
  }> {
    const isReasoning = options?.useReasoning ?? false
    const requestModel: OpenAIModel = isReasoning ? "o4-mini" : this.model
    const request: OpenAIRequest = {
      model: requestModel,
      messages: [
        {
          role: "user",
          content: prompt
        }
      ],
      max_tokens: options?.maxTokens || 1000,
      temperature: options?.temperature ?? (isReasoning ? undefined : 0.7),
      stream: false
    }
    
    if (options?.responseFormat) {
      request.response_format = options.responseFormat as OpenAIRequest["response_format"]
    } else if (options?.jsonMode) {
      request.response_format = { type: "json_object" }
    }
    
    // æ¨ç†æ¨¡å‹ä¸æ”¯æŒæŸäº›å‚æ•°
    if (isReasoning) {
      delete request.temperature
      delete request.response_format // o1 ç³»åˆ—ä¸æ”¯æŒ JSON Mode
    }
    
    try {
      const response = await fetch(this.endpoint, {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
          "Authorization": `Bearer ${this.config.apiKey}`
        },
        body: JSON.stringify(request),
        signal: options?.timeout 
          ? AbortSignal.timeout(options.timeout)
          : undefined
      })
      
      if (!response.ok) {
        const error = await response.text()
        throw new Error(`OpenAI API error (${response.status}): ${error}`)
      }
      
      const data: OpenAIResponse = await response.json()
      
      if (!data.choices || data.choices.length === 0) {
        throw new Error("OpenAI API returned no choices")
      }
      
      const content = data.choices[0].message.content
      
      // å¯¹äºæ¨ç†æ¨¡å‹ï¼Œå¯èƒ½éœ€è¦ç‰¹æ®Šå¤„ç†
      // o1 æ¨¡å‹ä¼šåœ¨ reasoning å­—æ®µè¿”å›æ€ç»´é“¾ï¼Œä½†æˆ‘ä»¬åªéœ€è¦æœ€ç»ˆç­”æ¡ˆ
      
      this.lastUsedModel = requestModel
      return {
        content,
        tokensUsed: {
          input: data.usage.prompt_tokens,
          output: data.usage.completion_tokens
        },
        model: requestModel
      }
    } catch (error) {
      if (error instanceof Error && error.name === 'AbortError') {
        throw new Error(`OpenAI API timeout (${options?.timeout}ms)`)
      }
      throw error
    }
  }
  
  /**
   * å®ç°ï¼šè®¡ç®—æˆæœ¬ï¼ˆäººæ°‘å¸ï¼‰
   */
  protected calculateCost(inputTokens: number, outputTokens: number): number {
    const pricing = MODEL_PRICING[this.lastUsedModel] || MODEL_PRICING["gpt-5-mini"]
    
    // å‡è®¾éƒ¨åˆ†è¾“å…¥ tokens å‘½ä¸­ç¼“å­˜
    const cachedTokens = Math.floor(inputTokens * this.CACHE_HIT_RATE)
    const uncachedTokens = inputTokens - cachedTokens
    
    // è®¡ç®—æˆæœ¬ï¼ˆç¾å…ƒï¼‰
    const inputCost = (uncachedTokens / 1_000_000) * pricing.input +
                     (cachedTokens / 1_000_000) * pricing.inputCached
    const outputCost = (outputTokens / 1_000_000) * pricing.output
    const totalCostUSD = inputCost + outputCost
    
    // è½¬æ¢ä¸ºäººæ°‘å¸ï¼ˆæ±‡ç‡çº¦ 7.2ï¼‰
    const totalCostCNY = totalCostUSD * 7.2
    
    openaiLogger.debug(`ğŸ’° æˆæœ¬è®¡ç®—: ${inputTokens} input (${cachedTokens} cached) + ${outputTokens} output = $${totalCostUSD.toFixed(4)} â‰ˆ Â¥${totalCostCNY.toFixed(4)}`)
    
    return totalCostCNY
  }

  protected getProfileResponseFormat(): Record<string, unknown> | null {
    return {
      type: "json_schema",
      json_schema: {
        name: "user_profile",
        strict: true,
        schema: {
          type: "object",
          additionalProperties: false,
          required: ["interests", "preferences", "avoidTopics"],
          properties: {
            interests: {
              type: "string",
              minLength: 20,
              maxLength: 400
            },
            preferences: {
              type: "array",
              minItems: 3,
              maxItems: 10,
              items: {
                type: "string",
                minLength: 2,
                maxLength: 80
              }
            },
            avoidTopics: {
              type: "array",
              minItems: 0,
              maxItems: 5,
              items: {
                type: "string",
                minLength: 2,
                maxLength: 60
              }
            }
          }
        }
      }
    }
  }
  
  /**
   * æµ‹è¯•è¿æ¥
   */
  async testConnection(): Promise<{
    success: boolean
    message: string
    latency?: number
  }> {
    try {
      const startTime = Date.now()
      
      await this.callChatAPI("æµ‹è¯•è¿æ¥ï¼Œè¯·å›å¤ OK", {
        maxTokens: 10,
        timeout: 10000,
        jsonMode: false
      })
      
      const latency = Date.now() - startTime
      
      return {
        success: true,
        message: `è¿æ¥æˆåŠŸï¼OpenAI API æ­£å¸¸å·¥ä½œ (æ¨¡å‹: ${this.model})`,
        latency
      }
    } catch (error) {
      return {
        success: false,
        message: `è¿æ¥å¤±è´¥: ${error instanceof Error ? error.message : String(error)}`
      }
    }
  }
}
