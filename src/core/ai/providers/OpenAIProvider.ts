/**
 * OpenAI Provider (Unified)
 * 
 * ç»Ÿä¸€çš„ OpenAI Providerï¼Œæ”¯æŒå¤šç§æ¨¡å‹ï¼š
 * 
 * æ ‡å‡†æ¨¡å‹ï¼ˆé€‚åˆæ—¥å¸¸å†…å®¹åˆ†æï¼‰ï¼š
 * - gpt-5-nano: æœ€å¿«æœ€ä¾¿å®œ ($0.050 è¾“å…¥, $0.400 è¾“å‡º)
 * - gpt-5-mini: å¹³è¡¡æ€§èƒ½å’Œæˆæœ¬ ($0.250 è¾“å…¥, $2.0 è¾“å‡º)
 * - gpt-5: æœ€å¼ºæ€§èƒ½ ($1.25 è¾“å…¥, $10.0 è¾“å‡º)
 * 
 * æ¨ç†æ¨¡å‹ï¼ˆé€‚åˆå¤æ‚å¤šæ­¥éª¤æ¨ç†ï¼‰ï¼š
 * - o4-mini: æ¨ç†æ¨¡å‹ï¼Œä¼šç”Ÿæˆæ€ç»´é“¾ ($4.0 è¾“å…¥, $16.0 è¾“å‡º)
 * 
 * ç‰¹ç‚¹ï¼š
 * - æ”¯æŒæç¤ºç¼“å­˜ï¼ˆPrompt Cachingï¼‰ï¼ŒèŠ‚çœæˆæœ¬
 * - OpenAI å…¼å®¹æ¥å£
 * - è‡ªåŠ¨æ ¹æ® model å‚æ•°é€‰æ‹©æ¨¡å‹
 * 
 * æ³¨ï¼šæœ¬å®ç°å‡è®¾ 10% ç¼“å­˜å‘½ä¸­ç‡è¿›è¡Œæˆæœ¬ä¼°ç®—
 */

import type {
  AIProvider,
  AIProviderConfig,
  UnifiedAnalysisResult,
  AnalyzeOptions,
  DeepSeekRequest,
  DeepSeekResponse,
  AIAnalysisOutput,
  UserProfileGenerationRequest,
  UserProfileGenerationResult
} from "@/types/ai"
import { logger } from "../../../utils/logger"

const openaiLogger = logger.withTag("OpenAIProvider")

// æ¨¡å‹å®šä»·ï¼ˆæ¯ 1M tokensï¼Œç¾å…ƒï¼‰
// æ•°æ®æ¥æºï¼šhttps://openai.com/api/pricing/ (2025-01)
const MODEL_PRICING = {
  "gpt-5-nano": {
    input: 0.050,
    inputCached: 0.005,
    output: 0.400
  },
  "gpt-5-mini": {
    input: 0.250,
    inputCached: 0.025,
    output: 2.0
  },
  "gpt-5": {
    input: 1.25,
    inputCached: 0.125,
    output: 10.0
  },
  "o4-mini": {
    // æ¨ç†æ¨¡å‹
    input: 4.0,
    inputCached: 1.0,
    output: 16.0
  }
} as const

type OpenAIModel = keyof typeof MODEL_PRICING

export class OpenAIProvider implements AIProvider {
  readonly name = "OpenAI"
  
  private config: AIProviderConfig
  private endpoint = "https://api.openai.com/v1/chat/completions"
  private model: OpenAIModel = "gpt-5-mini"
  
  constructor(config: AIProviderConfig) {
    this.config = config
    if (config.endpoint) {
      this.endpoint = config.endpoint
    }
    if (config.model && config.model in MODEL_PRICING) {
      this.model = config.model as OpenAIModel
    }
  }
  
  /**
   * æ£€æŸ¥æ˜¯å¦å¯ç”¨
   */
  async isAvailable(): Promise<boolean> {
    try {
      // æ£€æŸ¥ API Keyï¼ˆåªæ£€æŸ¥æ˜¯å¦å­˜åœ¨ï¼Œä¸é™åˆ¶é•¿åº¦ï¼‰
      if (!this.config.apiKey || this.config.apiKey.trim().length === 0) {
        openaiLogger.warn("API Key is empty")
        return false
      }
      
      // æ£€æŸ¥ç½‘ç»œï¼ˆç®€å•éªŒè¯ï¼‰
      if (typeof navigator !== 'undefined' && !navigator.onLine) {
        openaiLogger.warn("No network connection")
        return false
      }
      
      openaiLogger.debug(`âœ… OpenAI Provider is available (API Key: ${this.config.apiKey.substring(0, 10)}..., length: ${this.config.apiKey.length})`)
      return true
    } catch (error) {
      openaiLogger.error("isAvailable check failed:", error)
      return false
    }
  }
  
  /**
   * åˆ†æå†…å®¹
   */
  async analyzeContent(
    content: string,
    options?: AnalyzeOptions
  ): Promise<UnifiedAnalysisResult> {
    const startTime = Date.now()
    
    try {
      // 1. å†…å®¹é¢„å¤„ç†
      const processedContent = this.preprocessContent(content, options)
      
      // 2. æ„å»ºæç¤ºè¯ï¼ˆPhase 8: ä¼ é€’ç”¨æˆ·ç”»åƒï¼‰
      const prompt = this.buildPrompt(processedContent, options?.userProfile)
      
      // 3. è°ƒç”¨ OpenAI API
      const { response, actualModel } = await this.callAPI(prompt, options)
      
      // 4. è§£æå“åº”
      const analysis = this.parseResponse(response)
      
      // 5. è®¡ç®—æˆæœ¬ï¼ˆä½¿ç”¨å®é™…æ¨¡å‹ï¼‰
      const cost = this.calculateCost(
        response.usage.prompt_tokens,
        response.usage.completion_tokens,
        actualModel
      )
      
      // 6. è¿”å›ç»Ÿä¸€æ ¼å¼
      return {
        topicProbabilities: analysis.topics,
        metadata: {
          provider: "openai",
          model: actualModel,
          timestamp: Date.now(),
          tokensUsed: {
            prompt: response.usage.prompt_tokens,
            completion: response.usage.completion_tokens,
            total: response.usage.total_tokens
          },
          cost
        }
      }
    } catch (error) {
      openaiLogger.error("analyzeContent failed:", error)
      throw error
    }
  }
  
  /**
   * æµ‹è¯•è¿æ¥
   */
  async testConnection(): Promise<{
    success: boolean
    message: string
    latency?: number
  }> {
    const startTime = Date.now()
    
    try {
      // å‘é€æœ€å°è¯·æ±‚
      const request: DeepSeekRequest = {
        model: this.model,
        messages: [
          {
            role: "user",
            content: "Hello"
          }
        ],
        max_tokens: 10
      }
      
      const response = await fetch(this.endpoint, {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
          "Authorization": `Bearer ${this.config.apiKey}`
        },
        body: JSON.stringify(request),
        signal: AbortSignal.timeout(10000) // 10ç§’è¶…æ—¶
      })
      
      const latency = Date.now() - startTime
      
      if (!response.ok) {
        const error = await response.text()
        return {
          success: false,
          message: `API è¿”å›é”™è¯¯: ${response.status} ${error}`,
          latency
        }
      }
      
      return {
        success: true,
        message: `è¿æ¥æˆåŠŸï¼OpenAI ${this.model} API æ­£å¸¸å·¥ä½œ`,
        latency
      }
    } catch (error) {
      return {
        success: false,
        message: `è¿æ¥å¤±è´¥: ${error instanceof Error ? error.message : String(error)}`
      }
    }
  }
  
  /**
   * é¢„å¤„ç†å†…å®¹
   */
  private preprocessContent(content: string, options?: AnalyzeOptions): string {
    const maxLength = options?.maxLength || 3000
    
    // æˆªå–å†…å®¹
    let processed = content.substring(0, maxLength)
    
    // æ¸…ç†å¤šä½™ç©ºç™½
    processed = processed.replace(/\s+/g, " ").trim()
    
    return processed
  }
  
  /**
   * æ„å»ºæç¤ºè¯
   * 
   * Phase 8: æ”¯æŒä¼ é€’ç”¨æˆ·ç”»åƒè¿›è¡Œä¸ªæ€§åŒ–åˆ†æ
   */
  private buildPrompt(
    content: string,
    userProfile?: {
      interests: string
      preferences: string[]
      avoidTopics: string[]
    }
  ): string {
    // åˆ¤æ–­æ˜¯å¦ä¸ºæ¨ç†æ¨¡å‹
    const isReasoningModel = this.model.startsWith("o")
    
    // Phase 8: å¦‚æœæœ‰ç”¨æˆ·ç”»åƒï¼Œä½¿ç”¨ä¸ªæ€§åŒ– prompt
    if (userProfile && userProfile.interests) {
      if (isReasoningModel) {
        // æ¨ç†æ¨¡å‹ï¼šæ›´è¯¦ç»†çš„æç¤ºï¼Œå¼•å¯¼æ€è€ƒè¿‡ç¨‹
        return `ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½å†…å®¹åˆ†æåŠ©æ‰‹ï¼Œéœ€è¦æ ¹æ®ç”¨æˆ·å…´è¶£åˆ†ææ–‡ç« çš„ä¸»é¢˜å’Œç›¸å…³æ€§ã€‚

# ç”¨æˆ·ç”»åƒ
- **å…´è¶£é¢†åŸŸ**: ${userProfile.interests}
- **å†…å®¹åå¥½**: ${userProfile.preferences.join('ã€')}
- **é¿å…ä¸»é¢˜**: ${userProfile.avoidTopics.join('ã€')}

# æ–‡ç« å†…å®¹
${content}

è¯·ä»”ç»†æ€è€ƒï¼š
1. è¿™ç¯‡æ–‡ç« çš„ä¸»è¦ä¸»é¢˜æ˜¯ä»€ä¹ˆï¼Ÿ
2. å“ªäº›ä¸»é¢˜ä¸ç”¨æˆ·çš„å…´è¶£ç›¸å…³ï¼Ÿ
3. æ˜¯å¦åŒ…å«ç”¨æˆ·é¿å…çš„ä¸»é¢˜ï¼Ÿ
4. æ¯ä¸ªä¸»é¢˜çš„é‡è¦æ€§å¦‚ä½•ï¼Ÿ

ä»¥ JSON æ ¼å¼è¿”å›åˆ†æç»“æœï¼ŒåŒ…å«ä¸»é¢˜åŠå…¶æ¦‚ç‡ï¼ˆ0-1ä¹‹é—´çš„æ•°å­—ï¼Œæ€»å’Œä¸º1ï¼‰ã€‚
é¿å…çš„ä¸»é¢˜åº”è¯¥ç»™äºˆæ›´ä½çš„æ¦‚ç‡ã€‚

è¿”å›æ ¼å¼ç¤ºä¾‹ï¼š
{
  "topics": {
    "æŠ€æœ¯": 0.7,
    "å¼€æº": 0.2,
    "æ•™ç¨‹": 0.1
  }
}

åªè¿”å› JSONï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚`
      } else {
        // æ ‡å‡†æ¨¡å‹ï¼šç®€æ´æç¤º
        return `ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½å†…å®¹åˆ†æåŠ©æ‰‹ï¼Œéœ€è¦æ ¹æ®ç”¨æˆ·å…´è¶£åˆ†ææ–‡ç« çš„ä¸»é¢˜å’Œç›¸å…³æ€§ã€‚

# ç”¨æˆ·ç”»åƒ
- **å…´è¶£é¢†åŸŸ**: ${userProfile.interests}
- **å†…å®¹åå¥½**: ${userProfile.preferences.join('ã€')}
- **é¿å…ä¸»é¢˜**: ${userProfile.avoidTopics.join('ã€')}

# æ–‡ç« å†…å®¹
${content}

# åˆ†æè¦æ±‚
1. è¯†åˆ«æ–‡ç« çš„ 3-5 ä¸ªä¸»è¦ä¸»é¢˜
2. è¯„ä¼°æ¯ä¸ªä¸»é¢˜ä¸ç”¨æˆ·å…´è¶£çš„ç›¸å…³æ€§
3. ç»™å‡ºæ¯ä¸ªä¸»é¢˜çš„æ¦‚ç‡ï¼ˆ0-1ä¹‹é—´ï¼Œæ€»å’Œä¸º1ï¼‰
4. é¿å…çš„ä¸»é¢˜åº”è¯¥ç»™äºˆæ›´ä½çš„æ¦‚ç‡

# è¾“å‡ºæ ¼å¼ï¼ˆJSONï¼‰
{
  "topics": {
    "ä¸»é¢˜1": 0.5,
    "ä¸»é¢˜2": 0.3,
    "ä¸»é¢˜3": 0.2
  }
}

åªè¾“å‡º JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚`
      }
    }
    
    // é»˜è®¤ promptï¼ˆæ— ç”¨æˆ·ç”»åƒï¼‰
    if (isReasoningModel) {
      // æ¨ç†æ¨¡å‹ï¼šæ›´è¯¦ç»†çš„æç¤ºï¼Œå¼•å¯¼æ€è€ƒè¿‡ç¨‹
      return `ä½ æ˜¯ä¸€ä¸ªå†…å®¹åˆ†æä¸“å®¶ã€‚è¯·æ·±å…¥åˆ†æä»¥ä¸‹æ–‡æœ¬çš„ä¸»é¢˜åˆ†å¸ƒã€‚

æ–‡æœ¬å†…å®¹ï¼š
${content}

è¯·ä»”ç»†æ€è€ƒï¼š
1. è¿™ç¯‡æ–‡æœ¬ä¸»è¦è®¨è®ºä»€ä¹ˆè¯é¢˜ï¼Ÿ
2. æœ‰å“ªäº›æ¬¡è¦ä¸»é¢˜ï¼Ÿ
3. æ¯ä¸ªä¸»é¢˜å æ®å¤šå¤§æ¯”é‡ï¼Ÿ

ä»¥ JSON æ ¼å¼è¿”å›åˆ†æç»“æœï¼ŒåŒ…å«ä¸»é¢˜åŠå…¶æ¦‚ç‡ï¼ˆ0-1ä¹‹é—´çš„æ•°å­—ï¼Œæ€»å’Œä¸º1ï¼‰ã€‚
ä¸»é¢˜åº”è¯¥æ˜¯å…·ä½“çš„ã€æœ‰æ„ä¹‰çš„ç±»åˆ«ï¼ˆå¦‚"æŠ€æœ¯"ã€"è®¾è®¡"ã€"å•†ä¸š"ç­‰ï¼‰ã€‚

è¿”å›æ ¼å¼ç¤ºä¾‹ï¼š
{
  "topics": {
    "æŠ€æœ¯": 0.7,
    "å¼€æº": 0.2,
    "æ•™ç¨‹": 0.1
  }
}

åªè¿”å› JSONï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚`
    } else {
      // æ ‡å‡†æ¨¡å‹ï¼šç®€æ´æç¤º
      return `åˆ†æä»¥ä¸‹æ–‡æœ¬çš„ä¸»é¢˜åˆ†å¸ƒï¼Œè¾“å‡º JSON æ ¼å¼ç»“æœã€‚

æ–‡æœ¬ï¼š
${content}

è¯·è¯†åˆ« 3-5 ä¸ªä¸»è¦ä¸»é¢˜ï¼ˆå¦‚"æŠ€æœ¯"ã€"è®¾è®¡"ã€"å•†ä¸š"ç­‰ï¼‰ï¼Œå¹¶ç»™å‡ºæ¯ä¸ªä¸»é¢˜çš„æ¦‚ç‡ï¼ˆ0-1ä¹‹é—´ï¼Œæ€»å’Œä¸º1ï¼‰ã€‚

è¾“å‡ºæ ¼å¼ï¼ˆJSONï¼‰ï¼š
{
  "topics": {
    "æŠ€æœ¯": 0.6,
    "API": 0.3,
    "æ•™ç¨‹": 0.1
  }
}

åªè¾“å‡º JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚`
    }
  }
  
  /**
   * è°ƒç”¨ OpenAI API
   */
  private async callAPI(
    prompt: string,
    options?: AnalyzeOptions
  ): Promise<{ response: DeepSeekResponse; actualModel: OpenAIModel }> {
    // æ ¹æ®é…ç½®æˆ–å‚æ•°é€‰æ‹©æ¨¡å‹
    let selectedModel: OpenAIModel = this.model // ä½¿ç”¨å®ä¾‹çš„é»˜è®¤æ¨¡å‹
    
    // å¦‚æœæŒ‡å®šäº† useReasoningï¼Œè¦†ç›–é»˜è®¤æ¨¡å‹
    if (options?.useReasoning !== undefined) {
      if (options.useReasoning) {
        // ä½¿ç”¨æ¨ç†æ¨¡å‹
        selectedModel = "o4-mini"
      } else {
        // ä½¿ç”¨æ ‡å‡†æ¨¡å‹ï¼ˆå¦‚æœå½“å‰æ˜¯æ¨ç†æ¨¡å‹ï¼Œåˆ‡æ¢åˆ° gpt-5-miniï¼‰
        selectedModel = this.model.startsWith("o") ? "gpt-5-mini" : this.model
      }
    }
    
    openaiLogger.debug(`Using model: ${selectedModel}, useReasoning: ${options?.useReasoning}`)
    
    const request: DeepSeekRequest = {
      model: selectedModel,
      messages: [
        {
          role: "user",
          content: prompt
        }
      ],
      
      // æ¨ç†æ¨¡å‹ä¸æ”¯æŒ response_format
      ...(selectedModel.startsWith("o") ? {} : {
        // å¯ç”¨ JSON Modeï¼Œå¼ºåˆ¶æ¨¡å‹è¾“å‡º JSON
        response_format: {
          type: "json_object"
        }
      }),
      
      // max_tokens æ ¹æ®æ¨¡å‹ç±»å‹è°ƒæ•´
      max_tokens: selectedModel.startsWith("o") ? 4000 : 500,
      stream: false
    }
    
    // æ ¹æ®æ¨¡å‹ç±»å‹è®¾ç½®ä¸åŒè¶…æ—¶
    const defaultTimeout = selectedModel.startsWith("o") ? 120000 : 60000
    const timeout = options?.timeout || defaultTimeout
    
    openaiLogger.debug(`Timeout: ${timeout}ms for model ${selectedModel}`)
    
    const response = await fetch(this.endpoint, {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        "Authorization": `Bearer ${this.config.apiKey}`
      },
      body: JSON.stringify(request),
      signal: AbortSignal.timeout(timeout)
    })
    
    if (!response.ok) {
      const errorText = await response.text()
      throw new Error(`OpenAI API error: ${response.status} ${errorText}`)
    }
    
    const result = await response.json()
    
    // è¿”å›å“åº”å’Œå®é™…ä½¿ç”¨çš„æ¨¡å‹
    return { response: result, actualModel: selectedModel }
  }
  
  /**
   * è§£æå“åº”
   */
  private parseResponse(response: DeepSeekResponse): AIAnalysisOutput {
    try {
      const message = response.choices[0]?.message
      
      // æ¨ç†æ¨¡å‹å¯èƒ½æœ‰ reasoning_contentï¼ˆç±»ä¼¼ DeepSeekï¼‰
      const reasoningContent = (message as any)?.reasoning_content
      const finalContent = message?.content
      const finishReason = response.choices[0]?.finish_reason
      
      openaiLogger.debug("Response structure:", {
        hasReasoningContent: !!reasoningContent,
        hasFinalContent: !!finalContent,
        reasoningLength: reasoningContent?.length || 0,
        finalLength: finalContent?.length || 0,
        finishReason
      })
      
      // æ¨ç†å†…å®¹ä»…è®°å½•é•¿åº¦
      if (reasoningContent) {
        openaiLogger.debug(`Reasoning content length: ${reasoningContent.length} chars`)
      }
      
      // æ£€æŸ¥æ˜¯å¦æˆªæ–­
      if (finishReason === 'length') {
        openaiLogger.warn("Response truncated due to max_tokens limit")
      }
      
      // ä¼˜å…ˆä½¿ç”¨æœ€ç»ˆå›ç­”
      let content = finalContent
      
      // å¦‚æœæœ€ç»ˆå›ç­”ä¸ºç©ºï¼Œå°è¯•ä»æ¨ç†å†…å®¹æå–
      if (!content || content.trim().length === 0) {
        openaiLogger.warn("Final content is empty")
        
        if (reasoningContent && typeof reasoningContent === 'string') {
          openaiLogger.debug("Attempting to extract JSON from reasoning_content")
          content = reasoningContent
        } else {
          throw new Error("Both content and reasoning_content are empty")
        }
      }
      
      // æå– JSONï¼ˆå¤„ç†å¯èƒ½çš„ markdown ä»£ç å—ï¼‰
      const jsonMatch = content.match(/\{[\s\S]*\}/)
      if (!jsonMatch) {
        throw new Error("No JSON found in response")
      }
      
      const analysis = JSON.parse(jsonMatch[0]) as AIAnalysisOutput
      
      // éªŒè¯æ ¼å¼
      if (!analysis.topics || typeof analysis.topics !== "object") {
        throw new Error("Invalid topics format")
      }
      
      // å½’ä¸€åŒ–æ¦‚ç‡
      const topics = this.normalizeProbabilities(analysis.topics)
      
      return { topics }
    } catch (error) {
      openaiLogger.error("Failed to parse response:", error)
      openaiLogger.error("Response:", response)
      throw new Error(`è§£æ AI å“åº”å¤±è´¥: ${error instanceof Error ? error.message : String(error)}`)
    }
  }
  
  /**
   * å½’ä¸€åŒ–æ¦‚ç‡åˆ†å¸ƒ
   */
  private normalizeProbabilities(topics: Record<string, number>): Record<string, number> {
    const total = Object.values(topics).reduce((sum, prob) => sum + prob, 0)
    
    if (total === 0) {
      const count = Object.keys(topics).length
      return Object.fromEntries(
        Object.keys(topics).map(key => [key, 1 / count])
      )
    }
    
    return Object.fromEntries(
      Object.entries(topics).map(([key, prob]) => [key, prob / total])
    )
  }
  
  /**
   * Phase 8: ç”Ÿæˆç”¨æˆ·ç”»åƒ
   * 
   * ä½¿ç”¨ OpenAI Structured Outputs API ç¡®ä¿è¿”å›ç¨³å®šçš„ JSON æ ¼å¼
   */
  async generateUserProfile(
    request: UserProfileGenerationRequest
  ): Promise<UserProfileGenerationResult> {
    const startTime = Date.now()
    
    // 1. æ„å»ºä¸°å¯Œçš„ä¸Šä¸‹æ–‡
    const context = this.buildProfileContext(request)
    
    // 2. é€‰æ‹©æ¨¡å‹ï¼ˆä½¿ç”¨æ ‡å‡†æ¨¡å‹ï¼Œä¸ä½¿ç”¨æ¨ç†æ¨¡å‹ï¼‰
    // å¦‚æœå½“å‰é…ç½®æ˜¯æ¨ç†æ¨¡å‹ï¼Œåˆ‡æ¢åˆ° gpt-5-mini
    const selectedModel: OpenAIModel = this.model.startsWith("o") ? "gpt-5-mini" : this.model
    
    // 3. å®šä¹‰ JSON Schemaï¼ˆStructured Outputsï¼‰
    const responseFormat = {
      type: "json_schema",
      json_schema: {
        name: "user_profile",
        strict: true,
        schema: {
          type: "object",
          properties: {
            interests: {
              type: "string",
              description: "ç”¨æˆ·å…´è¶£æ€»ç»“ï¼Œ100-200å­—ï¼Œè¯¦ç»†å…·ä½“"
            },
            preferences: {
              type: "array",
              description: "åå¥½ç‰¹å¾åˆ—è¡¨ï¼Œ5-10æ¡",
              items: { type: "string" }
            },
            avoidTopics: {
              type: "array",
              description: "é¿å…ä¸»é¢˜åˆ—è¡¨ï¼Œ3-5æ¡",
              items: { type: "string" }
            }
          },
          required: ["interests", "preferences", "avoidTopics"],
          additionalProperties: false
        }
      }
    }
    
    // 4. è°ƒç”¨ OpenAI API
    const apiRequest = {
      model: selectedModel,
      messages: [{
        role: "user" as const,
        content: context.prompt
      }],
      temperature: 0.3,  // ä½æ¸©åº¦ï¼Œä¿è¯ä¸€è‡´æ€§
      max_tokens: 1000,
      response_format: responseFormat as any  // Structured Outputs
    }
    
    openaiLogger.debug(`Generating user profile with model: ${selectedModel}`)
    openaiLogger.debug(`Context: ${context.stats.totalBehaviors} behaviors, ${context.stats.topKeywords} top keywords`)
    
    try {
      // 5. å‘é€è¯·æ±‚
      const response = await fetch(this.endpoint, {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
          "Authorization": `Bearer ${this.config.apiKey}`
        },
        body: JSON.stringify(apiRequest),
        signal: AbortSignal.timeout(60000) // 60ç§’è¶…æ—¶
      })
      
      if (!response.ok) {
        const errorText = await response.text()
        throw new Error(`OpenAI API error: ${response.status} ${errorText}`)
      }
      
      const result: DeepSeekResponse = await response.json()
      
      // 6. è§£æç»“æœ
      const content = result.choices[0]?.message?.content
      if (!content) {
        throw new Error("Empty response from OpenAI")
      }
      
      const profile = JSON.parse(content)
      
      // 7. è®¡ç®—æˆæœ¬
      const usage = result.usage
      const cost = this.calculateCost(
        usage.prompt_tokens,
        usage.completion_tokens,
        selectedModel as OpenAIModel
      )
      
      const elapsed = Date.now() - startTime
      
      openaiLogger.info(`User profile generated in ${elapsed}ms`, {
        interests: profile.interests.slice(0, 50) + '...',
        preferences: profile.preferences.length,
        avoidTopics: profile.avoidTopics.length,
        cost: `Â¥${cost.toFixed(6)}`,
        tokens: `${usage.prompt_tokens} + ${usage.completion_tokens} = ${usage.total_tokens}`
      })
      
      return {
        interests: profile.interests,
        preferences: profile.preferences,
        avoidTopics: profile.avoidTopics,
        metadata: {
          provider: 'openai',
          model: selectedModel,
          timestamp: Date.now(),
          basedOn: {
            browses: request.behaviors.browses?.length || 0,
            reads: request.behaviors.reads?.length || 0,
            dismisses: request.behaviors.dismisses?.length || 0
          },
          tokensUsed: {
            prompt: usage.prompt_tokens,
            completion: usage.completion_tokens,
            total: usage.total_tokens
          },
          cost
        }
      }
    } catch (error) {
      openaiLogger.error("Failed to generate user profile:", error)
      throw error
    }
  }
  
  /**
   * æ„å»ºç”¨æˆ·ç”»åƒç”Ÿæˆçš„ Prompt
   */
  private buildProfileContext(request: UserProfileGenerationRequest): {
    prompt: string
    stats: {
      totalBehaviors: number
      topKeywords: number
    }
  } {
    const { behaviors, topKeywords } = request
    
    // å‡†å¤‡é˜…è¯»è®°å½•ï¼ˆæŒ‰æƒé‡æ’åºï¼Œå–å‰ 10ï¼‰
    const topReads = (behaviors.reads || [])
      .sort((a, b) => b.weight - a.weight)
      .slice(0, 10)
      .map((r, i) => 
        `${i + 1}. **${r.title}**\n` +
        `   æ‘˜è¦ï¼š${r.summary}\n` +
        `   æƒé‡ï¼š${r.weight.toFixed(2)}`
      )
      .join('\n\n')
    
    // å‡†å¤‡æ‹’ç»è®°å½•ï¼ˆå–å‰ 5ï¼‰
    const topDismisses = (behaviors.dismisses || [])
      .slice(0, 5)
      .map((d, i) =>
        `${i + 1}. **${d.title}**\n` +
        `   æ‘˜è¦ï¼š${d.summary}`
      )
      .join('\n\n')
    
    // å‡†å¤‡æµè§ˆè®°å½•ï¼ˆé«˜é¢‘å…³é”®è¯ï¼‰
    const keywordsSummary = topKeywords
      .slice(0, 20)
      .map((k, i) => `${i + 1}. ${k.word} (${k.count}æ¬¡)`)
      .join('\n')
    
    // æ„å»º Prompt
    const prompt = `
ä½ æ˜¯ç”¨æˆ·ç”»åƒåˆ†æä¸“å®¶ã€‚è¯·æ·±å…¥åˆ†æç”¨æˆ·çš„é˜…è¯»åå¥½ï¼Œç”Ÿæˆç²¾å‡†çš„å…´è¶£ç”»åƒã€‚

=== ğŸ“– ç”¨æˆ·é˜…è¯»è¿‡çš„æ¨èï¼ˆå¼ºçƒˆä¿¡å·ï¼‰===
${topReads || 'ï¼ˆæš‚æ— é˜…è¯»è®°å½•ï¼‰'}

=== âŒ ç”¨æˆ·æ‹’ç»çš„æ¨èï¼ˆè´Ÿå‘ä¿¡å·ï¼‰===
${topDismisses || 'ï¼ˆæš‚æ— æ‹’ç»è®°å½•ï¼‰'}

=== ğŸ”‘ é«˜é¢‘å…³é”®è¯ï¼ˆæµè§ˆè®°å½•ï¼‰===
${keywordsSummary || 'ï¼ˆæš‚æ— å…³é”®è¯ï¼‰'}

=== ğŸ“Š ç»Ÿè®¡ä¿¡æ¯ ===
- æ€»é˜…è¯»æ¨èï¼š${behaviors.reads?.length || 0} ç¯‡
- æ€»æ‹’ç»æ¨èï¼š${behaviors.dismisses?.length || 0} ç¯‡
- æµè§ˆå…³é”®è¯ï¼š${topKeywords.length} ä¸ª

=== ğŸ¯ åˆ†æä»»åŠ¡ ===
è¯·ç»¼åˆä»¥ä¸Šä¿¡æ¯ï¼Œç”Ÿæˆç”¨æˆ·ç”»åƒã€‚æ³¨æ„ï¼š
1. **ä¼˜å…ˆè€ƒè™‘é˜…è¯»è®°å½•**ï¼ˆæƒé‡æœ€é«˜ï¼Œä»£è¡¨ç”¨æˆ·çœŸå®åå¥½ï¼‰
2. **é‡è§†æ‹’ç»è®°å½•**ï¼ˆé¿å…æ¨èç±»ä¼¼å†…å®¹ï¼‰
3. **å‚è€ƒå…³é”®è¯**ï¼ˆè¾…åŠ©ç†è§£å…´è¶£å¹¿åº¦ï¼‰
4. **è¯†åˆ«ç»†åˆ†å…´è¶£**ï¼ˆä¸è¦åªå½’çº³åˆ°"æŠ€æœ¯"ã€"è®¾è®¡"ç­‰ç²—åˆ†ç±»ï¼‰
5. **æ•æ‰åå¥½é£æ ¼**ï¼ˆå¦‚"æ·±åº¦è§£æ" vs "å¿«é€Ÿå…¥é—¨"ï¼‰

è¯·ä¸¥æ ¼æŒ‰ç…§ JSON Schema è¿”å›ç»“æœã€‚
`.trim()
    
    return {
      prompt,
      stats: {
        totalBehaviors: (behaviors.reads?.length || 0) + (behaviors.dismisses?.length || 0),
        topKeywords: topKeywords.length
      }
    }
  }
  
  /**
   * è®¡ç®—æˆæœ¬ï¼ˆUSD â†’ CNYï¼Œè€ƒè™‘ç¼“å­˜ï¼Œå‡è®¾ 10% ç¼“å­˜å‘½ä¸­ç‡ï¼‰
   */
  private calculateCost(promptTokens: number, completionTokens: number, model: OpenAIModel): number {
    const cacheHitRate = 0.1
    const pricing = MODEL_PRICING[model]
    
    const inputCostCached = (promptTokens * cacheHitRate / 1_000_000) * pricing.inputCached
    const inputCostUncached = (promptTokens * (1 - cacheHitRate) / 1_000_000) * pricing.input
    const outputCost = (completionTokens / 1_000_000) * pricing.output
    
    // USD â†’ CNYï¼ˆæ±‡ç‡ 7.2ï¼‰
    const usdCost = inputCostCached + inputCostUncached + outputCost
    return usdCost * 7.2
  }
}
