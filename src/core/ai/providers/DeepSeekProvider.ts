/**
 * DeepSeek Provider V2 (é‡æ„ç‰ˆ)
 * 
 * ç»§æ‰¿ BaseAIServiceï¼Œåªå®ç° API è°ƒç”¨é€»è¾‘
 * æç¤ºè¯å’Œé€šç”¨é€»è¾‘ç”±åŸºç±»æä¾›
 */

import { BaseAIService } from "../BaseAIService"
import { DeepSeekCostCalculator, type TokenUsage } from "../CostCalculator"
import type { AIProviderConfig } from "@/types/ai"
import { logger, isNetworkError } from "@/utils/logger"

const deepseekLogger = logger.withTag("DeepSeekProvider")

// ä½¿ç”¨ç»Ÿä¸€çš„æˆæœ¬è®¡ç®—å™¨
const costCalculator = new DeepSeekCostCalculator()

/**
 * DeepSeek API è¯·æ±‚ç±»å‹
 */
interface DeepSeekRequest {
  model: string
  messages: Array<{
    role: "user" | "assistant" | "system"
    content: string
  }>
  response_format?: {
    type: "json_object"
  }
  max_tokens?: number
  stream?: boolean
}

/**
 * DeepSeek API å“åº”ç±»å‹
 */
interface DeepSeekResponse {
  choices: Array<{
    message: {
      role: string
      content: string
      reasoning_content?: string // DeepSeek æ¨ç†æ¨¡å¼ç‰¹æœ‰
    }
    finish_reason: string
  }>
  usage: {
    prompt_tokens: number
    completion_tokens: number
    total_tokens: number
    // DeepSeek ç¼“å­˜ç»Ÿè®¡å­—æ®µï¼ˆç”¨äºç²¾ç¡®è®¡è´¹ï¼‰
    prompt_cache_hit_tokens?: number   // ç¼“å­˜å‘½ä¸­çš„è¾“å…¥ tokens
    prompt_cache_miss_tokens?: number  // ç¼“å­˜æœªå‘½ä¸­çš„è¾“å…¥ tokens
  }
}

export class DeepSeekProvider extends BaseAIService {
  readonly name = "DeepSeek"
  
  private endpoint = "https://api.deepseek.com/v1/chat/completions"
  private model = "deepseek-chat"
  
  // æ¨ç†æ¨¡å¼ä½¿ç”¨çš„æ¨¡å‹
  private readonly REASONING_MODEL = "deepseek-reasoner"
  
  // è¿½è¸ªæœ€åä¸€æ¬¡è¯·æ±‚çš„ç¼“å­˜å‘½ä¸­æƒ…å†µï¼ˆç”¨äºç²¾ç¡®è®¡è´¹ï¼‰
  private lastCacheStats: {
    hitTokens: number
    missTokens: number
  } | null = null
  
  // è¿½è¸ªæœ€åä¸€æ¬¡è¯·æ±‚ä½¿ç”¨çš„æ¨¡å‹ï¼ˆç”¨äºæˆæœ¬è®¡ç®—ï¼‰
  private lastUsedModel = "deepseek-chat"
  
  constructor(config: AIProviderConfig) {
    super(config)
    this.model = (config.model as string) || this.model
    this.config.model = this.model
    
    if (config.endpoint) {
      this.endpoint = config.endpoint
    }
  }
  
  /**
   * å®ç°ï¼šè°ƒç”¨ DeepSeek Chat API
   */
  protected async callChatAPI(
    prompt: string,
    options?: {
      maxTokens?: number
      timeout?: number
      jsonMode?: boolean
      useReasoning?: boolean
      responseFormat?: Record<string, unknown>
      temperature?: number
    }
  ): Promise<{
    content: string
    tokensUsed: {
      input: number
      output: number
    }
    model?: string
  }> {
    // æ¨ç†æ¨¡å¼ä½¿ç”¨ deepseek-reasoner æ¨¡å‹
    const useReasoning = options?.useReasoning || false
    const actualModel = useReasoning ? this.REASONING_MODEL : this.model
    
    // è®°å½•ä½¿ç”¨çš„æ¨¡å‹ï¼ˆç”¨äºæˆæœ¬è®¡ç®—ï¼‰
    this.lastUsedModel = actualModel
    
    const request: DeepSeekRequest = {
      model: actualModel,
      messages: [
        {
          role: "user",
          content: prompt
        }
      ],
      max_tokens: options?.maxTokens || (useReasoning ? 64000 : 8000), // æ¨ç†æ¨¡å¼ä½¿ç”¨æœ€å¤§å€¼ 64Kï¼Œæ ‡å‡†æ¨¡å¼ 8K
      stream: false
    }
    
    // å¯ç”¨ JSON Modeï¼ˆdeepseek-reasoner ä¹Ÿæ”¯æŒ JSON è¾“å‡ºï¼‰
    if (options?.responseFormat) {
      request.response_format = options.responseFormat as { type: "json_object" }
    } else if (options?.jsonMode) {
      request.response_format = {
        type: "json_object"
      }
    }
    
    // Phase 12.6: ä½¿ç”¨é…ç½®çš„è¶…æ—¶ï¼ˆå¦‚æœæœªæŒ‡å®šï¼Œä½¿ç”¨ getConfiguredTimeoutï¼‰
    const timeout = options?.timeout || this.getConfiguredTimeout(useReasoning)
    
    try {
      const response = await fetch(this.endpoint, {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
          "Authorization": `Bearer ${this.config.apiKey}`
        },
        body: JSON.stringify(request),
        signal: AbortSignal.timeout(timeout)
      })
      
      if (!response.ok) {
        const errorText = await response.text()
        throw new Error(`DeepSeek API error: ${response.status} ${errorText}`)
      }
      
      const data = await response.json() as DeepSeekResponse
      
      // æå–å†…å®¹
      const message = data.choices[0]?.message
      let content = message?.content
      const reasoningContent = message?.reasoning_content
      
      // è°ƒè¯•æ—¥å¿—ï¼šæ˜¾ç¤ºåŸå§‹å“åº”
      const finishReason = data.choices[0]?.finish_reason
      deepseekLogger.debug("API å“åº”", {
        model: request.model,
        hasContent: !!content,
        contentLength: content?.length || 0,
        hasReasoningContent: !!reasoningContent,
        reasoningContentLength: reasoningContent?.length || 0,
        finishReason
      })
      
      // æ£€æŸ¥æ˜¯å¦å›  token é™åˆ¶è¢«æˆªæ–­
      if (finishReason === 'length') {
        deepseekLogger.warn("âš ï¸ å“åº”å›  max_tokens é™åˆ¶è¢«æˆªæ–­", {
          model: actualModel,
          maxTokens: request.max_tokens,
          tokensUsed: data.usage.total_tokens
        })
      }
      
      // æ¨ç†æ¨¡å¼ç‰¹æ®Šå¤„ç†ï¼šdeepseek-reasoner å¯èƒ½è¿”å›ç©º content
      // æ ¹æ®å®˜æ–¹æ–‡æ¡£ï¼ŒJSON mode æœ‰æ—¶ä¼šè¿”å›ç©º content
      if (!content && useReasoning && reasoningContent) {
        deepseekLogger.warn("âš ï¸ æ¨ç†æ¨¡å¼è¿”å›ç©º contentï¼Œå°è¯•ä» reasoning_content æå–")
        
        // å°è¯•ä» reasoning_content ä¸­æå– JSON
        // æ–¹æ³•1ï¼šæŸ¥æ‰¾ ```json ä»£ç å—
        const jsonMatch = reasoningContent.match(/```json\s*([\s\S]*?)\s*```/)
        if (jsonMatch) {
          content = jsonMatch[1].trim()
          deepseekLogger.info("âœ… ä» reasoning_content ä¸­æå–åˆ° JSON ä»£ç å—")
        } else {
          // æ–¹æ³•2ï¼šä»åå¾€å‰æŸ¥æ‰¾æœ€åä¸€ä¸ªå®Œæ•´çš„ JSON å¯¹è±¡ï¼ˆé¿å…æå–æç¤ºè¯ä¸­çš„ç¤ºä¾‹ï¼‰
          // æ¨ç†å†…å®¹é€šå¸¸æ˜¯ï¼šæ€è€ƒè¿‡ç¨‹ + æœ€ç»ˆ JSONï¼Œæˆ‘ä»¬éœ€è¦æœ€åé‚£ä¸ª JSON
          const lastBraceIndex = reasoningContent.lastIndexOf('}')
          if (lastBraceIndex !== -1) {
            // ä»æœ€åä¸€ä¸ª } å¾€å‰æ‰¾å¯¹åº”çš„ {
            let braceCount = 0
            let startIndex = -1
            
            for (let i = lastBraceIndex; i >= 0; i--) {
              if (reasoningContent[i] === '}') braceCount++
              if (reasoningContent[i] === '{') {
                braceCount--
                if (braceCount === 0) {
                  startIndex = i
                  break
                }
              }
            }
            
            if (startIndex !== -1) {
              content = reasoningContent.substring(startIndex, lastBraceIndex + 1)
              
              // éªŒè¯æå–çš„ JSON æ˜¯å¦æœ‰æ•ˆ
              try {
                JSON.parse(content)
                deepseekLogger.info("âœ… ä» reasoning_content ä¸­æå–åˆ° JSON å¯¹è±¡")
              } catch (e) {
                deepseekLogger.warn("âš ï¸ æå–çš„ JSON æ— æ•ˆï¼Œå¯èƒ½è¢«æˆªæ–­", { error: e instanceof Error ? e.message : String(e) })
                content = "" // é‡ç½®ï¼Œè§¦å‘é”™è¯¯
              }
            }
          }
        }
        
        // æ‰“å°æå–çš„å†…å®¹ç”¨äºè°ƒè¯•
        if (content) {
          deepseekLogger.debug("æå–çš„ JSON å†…å®¹é¢„è§ˆ:", content.substring(0, 500))
        }
      }
      
      if (!content) {
        const errorMsg = finishReason === 'length'
          ? "Response truncated due to max_tokens limit. Consider increasing max_tokens."
          : "Empty response from DeepSeek API"
        
        // ä»…åœ¨éæµ‹è¯•åœºæ™¯ï¼ˆmaxTokens > 200ï¼‰æ—¶æ‰è®°å½•æˆªæ–­è­¦å‘Š
        // æµ‹è¯•è¿æ¥æ—¶çš„æˆªæ–­æ˜¯é¢„æœŸè¡Œä¸ºï¼Œä¸åº”è¯¥æ˜¾ç¤ºè­¦å‘Š
        if (finishReason === 'length' && (request.max_tokens || 0) > 200) {
          deepseekLogger.warn("âš ï¸ å“åº”å›  max_tokens é™åˆ¶è¢«æˆªæ–­", {
            model: request.model,
            maxTokens: request.max_tokens,
            reasoningContentPreview: reasoningContent?.substring(0, 200)
          })
        }
        
        deepseekLogger.error("âŒ API è¿”å›ç©º content", {
          model: request.model,
          finishReason,
          maxTokens: request.max_tokens,
          reasoningContentPreview: reasoningContent?.substring(0, 200)
        })
        throw new Error(errorMsg)
      }
      
      // ä¿å­˜ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯ï¼ˆç”¨äºæˆæœ¬è®¡ç®—ï¼‰
      this.lastCacheStats = {
        hitTokens: data.usage.prompt_cache_hit_tokens || 0,
        missTokens: data.usage.prompt_cache_miss_tokens || data.usage.prompt_tokens // å¦‚æœæ²¡æœ‰ç¼“å­˜å­—æ®µï¼Œå…¨éƒ¨è§†ä¸ºæœªå‘½ä¸­
      }
      
      // æ—¥å¿—è®°å½•ç¼“å­˜å‘½ä¸­æƒ…å†µ
      if (data.usage.prompt_cache_hit_tokens !== undefined) {
        deepseekLogger.debug("ç¼“å­˜ç»Ÿè®¡", {
          hitTokens: this.lastCacheStats.hitTokens,
          missTokens: this.lastCacheStats.missTokens,
          hitRate: this.lastCacheStats.hitTokens / (this.lastCacheStats.hitTokens + this.lastCacheStats.missTokens) * 100
        })
      }
      
      return {
        content,
        tokensUsed: {
          input: data.usage.prompt_tokens,
          output: data.usage.completion_tokens
        },
        model: request.model
      }
    } catch (error) {
      // ç½‘ç»œé”™è¯¯æ˜¯ä¸´æ—¶æ€§çš„ï¼Œä½¿ç”¨ warn çº§åˆ«
      if (isNetworkError(error)) {
        deepseekLogger.warn("âš ï¸ API è°ƒç”¨å¤±è´¥ï¼ˆç½‘ç»œé—®é¢˜ï¼‰", error)
      } else {
        deepseekLogger.error("âŒ API è°ƒç”¨å¤±è´¥", error)
      }
      throw error
    }
  }
  
  /**
   * å®ç°ï¼šæµå¼è°ƒç”¨ DeepSeek Chat API
   * 
   * ç”¨äºæ¨ç†æ¨¡å¼ç­‰é•¿æ—¶é—´è¿è¡Œçš„ä»»åŠ¡ã€‚
   * ä½¿ç”¨ç©ºé—²è¶…æ—¶è€Œéæ€»æ—¶é—´è¶…æ—¶ï¼Œåªè¦æŒç»­æ”¶åˆ°æ•°æ®å°±ä¸ä¼šè¶…æ—¶ã€‚
   */
  protected override async callChatAPIStreaming(
    prompt: string,
    options?: {
      maxTokens?: number
      idleTimeout?: number
      jsonMode?: boolean
      useReasoning?: boolean
      responseFormat?: Record<string, unknown>
      temperature?: number
    }
  ): Promise<{
    content: string
    tokensUsed: {
      input: number
      output: number
    }
    model?: string
  }> {
    const useReasoning = options?.useReasoning || false
    const actualModel = useReasoning ? this.REASONING_MODEL : this.model
    const idleTimeout = options?.idleTimeout || 60000 // é»˜è®¤ 60 ç§’ç©ºé—²è¶…æ—¶
    
    this.lastUsedModel = actualModel
    
    const request: DeepSeekRequest = {
      model: actualModel,
      messages: [{ role: "user", content: prompt }],
      max_tokens: options?.maxTokens || (useReasoning ? 64000 : 8000),
      stream: true  // å¯ç”¨æµå¼è¾“å‡º
    }
    
    // å¯ç”¨ JSON Mode
    if (options?.responseFormat) {
      request.response_format = options.responseFormat as { type: "json_object" }
    } else if (options?.jsonMode) {
      request.response_format = { type: "json_object" }
    }
    
    deepseekLogger.debug("ğŸŒŠ å¼€å§‹æµå¼è°ƒç”¨", {
      model: actualModel,
      maxTokens: request.max_tokens,
      idleTimeout,
      useReasoning
    })
    
    try {
      const response = await fetch(this.endpoint, {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
          "Authorization": `Bearer ${this.config.apiKey}`
        },
        body: JSON.stringify(request)
        // æ³¨æ„ï¼šæµå¼è°ƒç”¨ä¸è®¾ç½®æ€»è¶…æ—¶ï¼Œä¾èµ–ç©ºé—²è¶…æ—¶
      })
      
      if (!response.ok) {
        const errorText = await response.text()
        throw new Error(`DeepSeek API error: ${response.status} ${errorText}`)
      }
      
      if (!response.body) {
        throw new Error("Response body is null")
      }
      
      const reader = response.body.getReader()
      const decoder = new TextDecoder()
      let fullContent = ''
      let reasoningContent = ''
      let usage = { input: 0, output: 0 }
      let buffer = ''  // ç”¨äºå¤„ç†è·¨ chunk çš„æ•°æ®
      
      // è¿›åº¦è¿½è¸ª
      let lastProgressLog = 0
      const PROGRESS_LOG_INTERVAL = 2000  // æ¯ 2000 å­—ç¬¦è¾“å‡ºä¸€æ¬¡è¿›åº¦
      const streamStartTime = Date.now()
      let chunkCount = 0
      
      // ç©ºé—²è¶…æ—¶æ§åˆ¶
      let idleTimer: ReturnType<typeof setTimeout> | null = null
      let timedOut = false
      
      const resetIdleTimer = () => {
        if (idleTimer) clearTimeout(idleTimer)
        idleTimer = setTimeout(() => {
          timedOut = true
          reader.cancel()
        }, idleTimeout)
      }
      
      // å¯åŠ¨ç©ºé—²è®¡æ—¶å™¨
      resetIdleTimer()
      
      try {
        while (true) {
          const { done, value } = await reader.read()
          
          if (timedOut) {
            throw new Error(`Idle timeout: no data received for ${idleTimeout}ms`)
          }
          
          if (done) break
          
          // æ”¶åˆ°æ•°æ®ï¼Œé‡ç½®ç©ºé—²è®¡æ—¶å™¨
          resetIdleTimer()
          chunkCount++
          
          // è§£ç å¹¶å¤„ç† SSE æ•°æ®
          buffer += decoder.decode(value, { stream: true })
          const lines = buffer.split('\n')
          
          // ä¿ç•™æœ€åä¸€è¡Œï¼ˆå¯èƒ½ä¸å®Œæ•´ï¼‰
          buffer = lines.pop() || ''
          
          for (const line of lines) {
            if (!line.startsWith('data: ')) continue
            
            const data = line.slice(6).trim()
            if (data === '[DONE]') continue
            
            try {
              const parsed = JSON.parse(data)
              const delta = parsed.choices?.[0]?.delta
              
              // æå–å†…å®¹
              if (delta?.content) {
                fullContent += delta.content
              }
              
              // æå–æ¨ç†å†…å®¹ï¼ˆDeepSeek ç‰¹æœ‰ï¼‰
              if (delta?.reasoning_content) {
                reasoningContent += delta.reasoning_content
              }
              
              // ğŸ“Š è¿›åº¦æ—¥å¿—ï¼šæ¯ PROGRESS_LOG_INTERVAL å­—ç¬¦è¾“å‡ºä¸€æ¬¡
              const totalReceived = fullContent.length + reasoningContent.length
              if (totalReceived - lastProgressLog >= PROGRESS_LOG_INTERVAL) {
                const elapsed = ((Date.now() - streamStartTime) / 1000).toFixed(1)
                deepseekLogger.info(`ğŸŒŠ æµå¼æ¥æ”¶ä¸­...`, {
                  elapsed: `${elapsed}s`,
                  contentChars: fullContent.length,
                  reasoningChars: reasoningContent.length,
                  chunks: chunkCount
                })
                lastProgressLog = totalReceived
              }
              
              // æå– usageï¼ˆæœ€åä¸€ä¸ª chunkï¼‰
              if (parsed.usage) {
                usage = {
                  input: parsed.usage.prompt_tokens || 0,
                  output: parsed.usage.completion_tokens || 0
                }
                
                // ä¿å­˜ç¼“å­˜ç»Ÿè®¡
                this.lastCacheStats = {
                  hitTokens: parsed.usage.prompt_cache_hit_tokens || 0,
                  missTokens: parsed.usage.prompt_cache_miss_tokens || parsed.usage.prompt_tokens || 0
                }
              }
            } catch {
              // å¿½ç•¥è§£æé”™è¯¯ï¼ˆå¯èƒ½æ˜¯ä¸å®Œæ•´çš„ JSONï¼‰
            }
          }
        }
      } finally {
        if (idleTimer) clearTimeout(idleTimer)
      }
      
      // è®¡ç®—æ€»è€—æ—¶
      const totalDuration = ((Date.now() - streamStartTime) / 1000).toFixed(1)
      
      // æ¨ç†æ¨¡å¼ç‰¹æ®Šå¤„ç†ï¼šå¯èƒ½éœ€è¦ä» reasoning_content æå– JSON
      if (!fullContent && useReasoning && reasoningContent) {
        deepseekLogger.warn("âš ï¸ æµå¼æ¨ç†æ¨¡å¼è¿”å›ç©º contentï¼Œå°è¯•ä» reasoning_content æå–")
        fullContent = this.extractJsonFromReasoning(reasoningContent)
      }
      
      if (!fullContent) {
        throw new Error("Empty response from streaming API")
      }
      
      // ğŸ“Š å®Œæˆæ—¥å¿—ï¼šæ˜¾ç¤ºå®Œæ•´ç»Ÿè®¡
      deepseekLogger.info("âœ… æµå¼è°ƒç”¨å®Œæˆ", {
        duration: `${totalDuration}s`,
        contentChars: fullContent.length,
        reasoningChars: reasoningContent.length,
        totalChunks: chunkCount,
        tokensUsed: usage
      })
      
      return {
        content: fullContent,
        tokensUsed: usage,
        model: actualModel
      }
    } catch (error) {
      if (isNetworkError(error)) {
        deepseekLogger.warn("âš ï¸ æµå¼è°ƒç”¨å¤±è´¥ï¼ˆç½‘ç»œé—®é¢˜ï¼‰", error)
      } else {
        deepseekLogger.error("âŒ æµå¼è°ƒç”¨å¤±è´¥", error)
      }
      throw error
    }
  }
  
  /**
   * ä» reasoning_content ä¸­æå– JSON
   */
  private extractJsonFromReasoning(reasoningContent: string): string {
    // æ–¹æ³•1ï¼šæŸ¥æ‰¾ ```json ä»£ç å—
    const jsonMatch = reasoningContent.match(/```json\s*([\s\S]*?)\s*```/)
    if (jsonMatch) {
      deepseekLogger.info("âœ… ä» reasoning_content ä¸­æå–åˆ° JSON ä»£ç å—")
      return jsonMatch[1].trim()
    }
    
    // æ–¹æ³•2ï¼šä»åå¾€å‰æŸ¥æ‰¾æœ€åä¸€ä¸ªå®Œæ•´çš„ JSON å¯¹è±¡
    const lastBraceIndex = reasoningContent.lastIndexOf('}')
    if (lastBraceIndex !== -1) {
      let braceCount = 0
      let startIndex = -1
      
      for (let i = lastBraceIndex; i >= 0; i--) {
        if (reasoningContent[i] === '}') braceCount++
        if (reasoningContent[i] === '{') {
          braceCount--
          if (braceCount === 0) {
            startIndex = i
            break
          }
        }
      }
      
      if (startIndex !== -1) {
        const extracted = reasoningContent.substring(startIndex, lastBraceIndex + 1)
        try {
          JSON.parse(extracted)
          deepseekLogger.info("âœ… ä» reasoning_content ä¸­æå–åˆ° JSON å¯¹è±¡")
          return extracted
        } catch {
          deepseekLogger.warn("âš ï¸ æå–çš„ JSON æ— æ•ˆ")
        }
      }
    }
    
    return ''
  }
  
  /**
   * å®ç°ï¼šè·å–è´§å¸ç±»å‹
   */
  protected getCurrency(): 'CNY' | 'USD' | 'FREE' {
    return 'CNY'  // DeepSeek ä½¿ç”¨äººæ°‘å¸
  }

  /**
   * å®ç°ï¼šè®¡ç®—æˆæœ¬
   * 
   * ä½¿ç”¨ç»Ÿä¸€çš„ CostCalculator è®¡ç®—æˆæœ¬
   */
  protected calculateCost(inputTokens: number, outputTokens: number): number {
    const breakdown = this.calculateCostBreakdown(inputTokens, outputTokens)
    return breakdown.input + breakdown.output
  }
  
  /**
   * å®ç°ï¼šè®¡ç®—æˆæœ¬æ˜ç»†ï¼ˆè¾“å…¥å’Œè¾“å‡ºåˆ†å¼€ï¼‰
   * 
   * ä½¿ç”¨ API è¿”å›çš„çœŸå®ç¼“å­˜å‘½ä¸­æ•°æ®è®¡ç®—æˆæœ¬
   */
  protected calculateCostBreakdown(inputTokens: number, outputTokens: number): { input: number; output: number } {
    // æ„å»º TokenUsage å¯¹è±¡
    const usage: TokenUsage = {
      input: inputTokens,
      output: outputTokens,
      // å¦‚æœæœ‰ç¼“å­˜æ•°æ®ï¼Œä½¿ç”¨çœŸå®çš„ç¼“å­˜å‘½ä¸­æ•°
      cachedInput: this.lastCacheStats?.hitTokens
    }
    
    // ä½¿ç”¨ç»Ÿä¸€çš„æˆæœ¬è®¡ç®—å™¨
    const result = costCalculator.calculateCost(usage, this.lastUsedModel)
    return { input: result.input, output: result.output }
  }
  
  /**
   * æµ‹è¯•è¿æ¥
   */
  async testConnection(): Promise<{
    success: boolean
    message: string
    latency?: number
  }> {
    try {
      const startTime = Date.now()
      
      // ä½¿ç”¨ç®€çŸ­æç¤ºè¯å¹¶è®¾ç½®è¶³å¤Ÿçš„ maxTokens é¿å…æˆªæ–­è­¦å‘Š
      // ä¸­æ–‡"å›å¤OK"é€šå¸¸åªéœ€è¦å‡ ä¸ª tokenï¼Œä½† API å¯èƒ½è¿”å›è¾ƒé•¿çš„å‹å¥½å“åº”
      await this.callChatAPI("å›å¤OKå³å¯", {
        maxTokens: 50,
        timeout: 10000,
        jsonMode: false
      })
      
      const latency = Date.now() - startTime
      
      return {
        success: true,
        message: `è¿æ¥æˆåŠŸï¼DeepSeek API æ­£å¸¸å·¥ä½œ`,
        latency
      }
    } catch (error) {
      return {
        success: false,
        message: `è¿æ¥å¤±è´¥: ${error instanceof Error ? error.message : String(error)}`
      }
    }
  }
}
